https://doi.org/10.1177/03009858241286828
Veterinary Pathology
2025, Vol. 62(2) 139 –151
© The Author(s) 2024
Article reuse guidelines: 
sagepub.com/journals-permissions
DOI: 10.1177/03009858241286828
journals.sagepub.com/home/vet
Oncology – Original Article
Feline chronic enteropathy (FCE) is a common cause of mor -
bidity and mortality in old cats.14 Histologic quantification and 
localization of lymphocyte infiltrates in small intestinal biop-
sies is an integral part of distinguishing chronic enteritis from 
low-grade intestinal lymphoma and to assess the severity of the 
condition.14 However, healthy cats and cats with FCE can have 
similar histologic characteristics, and criteria for separating 
inflammation from low-grade intestinal lymphoma are conten-
tious.14 In addition, multiple studies have demonstrated low 
interobserver agreement for pathologists grading of lympho-
cyte infiltrates in cats, despite the use of standardized semi-
quantitative grading schemes.29 The inherent low reproducibility 
of histologic grading undermines the statistical power of 
research studies and contributes to skepticism among clinicians 
regarding the utility of biopsies for the diagnosis of FCE.
Recent advancements in whole-slide imaging and artificial 
intelligence (AI) have facilitated the development of AI models 
for histopathology.2 Convolutional neural networks, the most 
common type of AI algorithm for advanced image analysis, 
have shown promise in improving the reproducibility of histologic 
evaluations in diagnostic practice.2 AI models for histopathol-
ogy can be employed at the image level to predict a diagnosis 
(image classification tasks), at the object level to identify 
1286828 VETXXX10.1177/03009858241286828Veterinary PathologyWulcan et al
research-article2024
1University of California, Davis, Davis, CA
2Texas A&M University, College Station, TX
3University of Surrey, Guildford, UK
4University of Bern, Bern, Switzerland Analysis results were merged during postprocessing. Performance 
was validated at the lymphocyte level and concordance with 
pathologist grades was assessed at the whole-slide image (WSI) level.
Figure 2. Case material. Randomly selected slides were, after 
excluding inadmissible slides, partitioned into training, tuning, and 
test sets. 140 Veterinary Pathology 62(2) 
specific image elements (object detection tasks), or at the pixel 
level to predict a label for each pixel (segmentation tasks).13 In 
contrast to image level AI models and semiquantitative grading 
by a pathologist, object detection models and semantic seg-
mentation models enable the extraction of quantitative features 
from histology slides. This allows the identification of histo-
logic patterns and facilitates the integration of histology results 
into multimodal analyses.
The objective of this study was to develop and validate an 
AI model to quantify intraepithelial and lamina propria lym-
phocytes in hematoxylin and eosin (H&E)-stained small intes-
tinal biopsies from cats. The ultimate purpose of the model is to 
enhance the diagnostic accuracy and consistency of histologic 
diagnoses for FCE by providing a reproducible, objective, and 
quantitative assessment of feline intestinal lymphocytes at the 
whole-slide level.
Materials and Methods
Model Design
To quantify intraepithelial and lamina propria lymphocytes, we 
developed 2 separate models and merged the model outputs 
(Fig. 1): a “cell detection” model that utilized object detection 
with instance segmentation to detect lymphocytes and identify 
lymphocyte nucleus boundaries, and a “mucosal compartment” 
model that used semantic segmentation to classify pixels into 
epithelium, lamina propria, intestinal lumen, or other. In a post-
analysis step, lymphocytes were then classified as intraepithe-
lial or lamina propria lymphocytes depending on whether the 
center of a lymphocyte was located within the epithelium or 
lamina propria. Both models were strongly supervised convo-
lutional neural networks, developed using Aiforia create 
(Helsinki, Finland). Settings and versions are detailed in 
Supplemental Table S1.
Case Material
The AI models were developed and tested using retrospectively 
obtained feline small intestinal biopsies that were submitted for Wulcan et al 141
tuning sets. In each training and tuning set slide, a single region 
of interest (ROI) per model was annotated. The ROIs were 
selected by evaluating the small intestinal fragments for the 
most complete wall layers, and if multiple fragments met this 
criterion, one was randomly chosen. Detailed methodology for 
set size calculation, ROI selection, and annotations are avail-
able in the Supplemental Materials. Examples of training anno-
tations and test set analysis results are provided in Supplemental 
Figure S1. The test set whole-slide images (WSIs) were catego-
rized by a board-certified veterinary anatomic pathologist 
(J.M.W.) based on stain quality (adequate vs faded), tissue 
quality (adequate vs crushed), and image focus (adequate vs 
out of focus). Submitting laboratory, biopsy types, and quality 
features of the test set are presented in Fig. 3. Detailed criteria 
for assessing the quality of the test set slides are provided in the 
Supplemental Materials. Information regarding the number 
and size of training and tuning set annotations for each model 
is available in Supplemental Table S3.
Digitization
WSIs were generated at the Virtual Slide Scanning Facility, 
School of Veterinary Medicine, University of California Davis 
using an Olympus VS120 virtual slide microscope and a 40× 
objective (0.17 μm2/pixel). WSIs with suboptimal focus were 
purposefully not rescanned. Raw WSIs in the Olympus VSI 
format were deidentified and uploaded to Aiforia create 
(Helsinki, Finland).
Statistical Software
All postanalysis processing, statistical validation, and visual-
izations were conducted using R programming language within 
the RStudio integrated development environment.23,25 The cus-
tom R scripts were supported by a range of open-source pack-
ages for data science,26,27 data import and export, 19,20,28 spatial 
analysis,5,9,21 statistics,16 and visualization.3,11,18,22 All custom R intestinal biopsies, but their application for this study came 
with several major limitations. First, existing grading schemes 
are semiquantitative, while our AI model provides quantitative 
output with no established rules for translation. Second, these 
grading schemes were established for duodenal biopsies only, 
whereas our AI model was trained on various small intestinal 
segments. Third, the lamina propria grading scheme combines 
lymphocytes and plasma cells whereas our model is intended 
for quantifying lymphocytes only. In addition to these limita-
tions inherent to the grading scheme, shortfalls of our model 
likely contributed to the overlap of lymphocyte counts between 
different semiquantitative grades. First, the AI-generated lym-
phocyte counts were inaccurate for some WSIs. Second, 
AI-generated lymphocyte counts represented an average across 
a tissue fragment while pathologists may have considered 
regional differences in density of lymphocyte distribution. 
However, the fact that pathologists provided divergent grades 
for the same slide in the great majority of cases suggests that 
the limitations of semiquantitative grading, and a human’s abil-
ity to accurately and reproducibly estimate cell numbers might 
be a bigger source of error than the shortfalls of our model.
The most recent American College of Veterinary Internal 
Medicine guidelines for diagnosing and distinguishing low-
grade intestinal lymphoma from lymphoplasmacytic enteritis in 
FCE, expands on previous recommendations for grading small 
intestinal biopsies in cats.15 The guidelines emphasize the Wulcan et al 151
 9. Hahsler M, Piekenbrock M,  Doran D. Dbscan: fast density-based clustering 
with R. J Stat Softw. 2019;91:1–30.
 10. Ii T, Chambers JK, Nakashima K, et al. Intraepithelial lymphocytes are associ-
ated with epithelial injury in feline intestinal T-cell lymphoma. J Vet Med Sci. 
2024;86(1):101–110.
 11. Kolde R. pheatmap: pretty heatmaps. Published 2019. Accessed September 24, 
2024. https://CRAN.R-project.org/package=pheatmap.
 12. Kos Z, Roblin E, Kim RS, et al. Pitfalls in assessing stromal tumor infiltrat-
ing lymphocytes (sTILs) in breast cancer. NPJ Breast Cancer. 2020;6(1): 
1–16.
 13. Maier-Hein L, Reinke A, Godau P, et al. Metrics reloaded: recommendations 
for image analysis validation. Nat Methods. 2024;21(2):195–212.
 14. Marsilio S. Differentiating inflammatory bowel disease from alimentary lym-
phoma in cats: does it matter? Vet Clin Small Anim. 2021;51(1):93–109.
 15. Marsilio S, Freiche V, Johnson E, et al. ACVIM consensus statement 
guidelines on diagnosing and distinguishing low-grade neoplastic from 
inflammatory lymphocytic chronic enteropathies in cats. J Vet Intern Med. 
2023;37(3):794–816.
 16. Meyer D, Zeileis A,  Hornik K. The strucplot framework: visualizing multi-
way contingency tables with vcd. J Stat Softw. 2007;17:1–48.
 17. Najdawi F, Sucipto K, Mistry P, et al. Artificial intelligence enables quan-
titative assessment of ulcerative colitis histology. Mod Pathol. 2023;36(6): 
100124.
 18. Neuwirth E. RColorBrewer: ColorBrewer palettes. Published 2022. Accessed 
September 24, 2024. https://CRAN.R-project.org/package=RColorBrewer.
 19. Ooms J. The jsonlite package: a practical and consistent mapping between 
JSON data and R objects. arXiv. 2014. Accessed September 24, 2024. http://
arxiv.org/abs/1403.2805.
 20. Ooms J. writexl: export data framesto excel “xlsx” format. Published 2023. 
Accessed September 24, 2024. https://CRAN.R-project.org/package=writexl. ages for data science,26,27 data import and export, 19,20,28 spatial 
analysis,5,9,21 statistics,16 and visualization.3,11,18,22 All custom R 
scripts used in this study are available at GitHub (https://github.
com/ucdavis/AIFeBx_supplemental). An online random num-
ber generator was used for ROI selection during model devel-
opment (Supplemental Materials).8
Postanalysis Processing
To classify the lymphocytes as either “intraepithelial” or “lam-
ina propria,” output coordinates of both models were exported 
and merged. Each lymphocyte was mapped to a mucosal com-
partment based on the location of the lymphocyte centroid. The 
confidence score of lymphocytes mapped to a mucosal com-
partment was determined as follows. In addition to output coor-
dinates for AI-predictions, both models provided a confidence 
score for every pixel/object that reflects the probability of the 
Table 1. Original diagnosis by histology, immunohistochemistry, and clonality.
Training Tuning Test
T-cell lymphoma, smalla 79/124 (64%) 74/104 (71%) 75/105 (71%)
T-cell lymphoma, intermediate/largeb 3/124 (2%) 3/104 (3%) 3/105 (3%)
Enteritis 40/124 (32%) 27/104 (26%) 26/105 (25%)
Normal - - 1/105 (1%)
Nondiagnostic 2/124 (2%) - -
aIncludes presumed and emerging T-cell lymphomas.
bIncludes large granular lymphocyte lymphomas.
Figure 3. Composition of the test set (n = 105). (a) Submitting laboratory. Randomly selected slides were stratified by submitting laboratory 
resulting in a test set diverse in terms of origin. (b) Biopsy type. Slides were included in the study regardless of biopsy type. (c) Quality. Slides 
were included in the study regardless of quality, and rescanning of whole-slide images were purposefully not attempted. Supplemental Table S1.
Case Material
The AI models were developed and tested using retrospectively 
obtained feline small intestinal biopsies that were submitted for 
clonality testing to the Leukocyte Antigen Biology Laboratory 
at the School of Veterinary Medicine, University of California 
Davis between 2010 and 2020. A total of 383 cases from 11 
different laboratories were sampled and 1 H&E-stained slide 
per case was randomly selected for digitization (Fig. 2). Cases 
were included regardless of diagnosis, biopsy type, or slide 
quality. Table 1 contains a summary of the original diagnoses 
(based on histology, immunohistochemistry, and clonality 
assessments) for slides included in the training, tuning, and test 
sets. Supplemental Table S2 contains signalments and original 
diagnoses for all cases in the study. Cases where slides were 
missing, lacked small intestinal tissue, or failed scanning were 
excluded (n = 50). The remaining slides (n = 333) were divided 
into training (124 slides), tuning (104 slides), and testing (105 
slides) sets. The optimal tuning and test set sizes were deter-
mined based on a small pilot study that evaluated performance 
variability across slides (data not shown). The number of train-
ing slides were chosen based on the model performance in the 
Figure 1. Model design. Two distinct artificial intelligence models 
were iteratively trained. Between training rounds, intermediate 
versions underwent tuning using a separate set of slides. If tuning 
results were unsatisfactory, training sizes were increased or 
hyperparameters were adjusted. Once tuning set performances 
were deemed satisfactory, the models were finalized and applied to 
manually outlined sections of small intestine in the test set slides. 
Analysis results were merged during postprocessing. Performance 
was validated at the lymphocyte level and concordance with 
pathologist grades was assessed at the whole-slide image (WSI) level. in humans.17 The model-derived data correlated well with 
human semiquantitative grades and illustrates the potential 
benefit of AI-based cell quantification.17
Validation of our model by comparing AI-generated lym-
phocyte predictions to pathologist annotations in small valida-
tion regions revealed a low interobserver agreement for 
lymphocyte annotations between pathologists. Only 12% of all 
candidate lymphocytes were identified by all 11 pathologists 
and only 43% were annotated by the majority of annotators, 
suggesting that distinguishing lymphocytes from other cell 
types is not straightforward. The inconsistent identification of 
lymphocytes has been recognized as a source of interobserver 
variability for grading of tumor-infiltrating lymphocytes in 
human breast cancer but not for grading of feline intestinal 
biopsies.12 Given the high variability of annotations across 
pathologists in this study, determining the level of agreement 
required for identifying reference lymphocytes presented an Instances of unequivocal false negatives were infrequent in 
our study and primarily occurred in WSIs with faded stain or 
suboptimal focus. Our model development strategy leveraged a 
data set comprised of slides from several diagnostic laborato-
ries, deliberately incorporating variations in image quality and 
refraining from rescanning WSIs with suboptimal focus. In 
contrast to the common practice of developing AI models on 
extensively filtered and cleaned data sets,24 our approach aimed 
to maintain diversity. This decision potentially resulted in a 
lower test performance but will likely mitigate the expected 
performance drop when transitioning the model into clinical 
use. With that said, faded stain, while common in archived case 
material, is not expected in routine diagnostic practice, and out 
of focus WSIs would have triggered rescanning in a diagnostic 
laboratory. Limiting the use of the AI model to biopsies of ade-
quate stain quality and image focus is expected to reduce the 
frequency of false-negative predictions and improve sensitivity 
without the need for further model improvement.
For this study, we calculated performance metrics separately 
for each validation region and averaged the results across the 
test set. In contrast to pooling of results from all validation 
regions and calculating a single value for each performance 
metric, our strategy allows for the assessment of performance 
variability between different WSIs. Choosing smaller validation 
regions and more slides rather than bigger regions and fewer 
slides was motivated by providing a sufficiently small area for 
manual annotation while maximizing the diversity of the tissue 
assessed. However, this strategy likely contributed to the vari-
ability in performance observed between different WSIs.
When comparing semiquantitative grades between patholo-
gists for WSIs, we observed a low interobserver agreement. The 
current World Small Animal Veterinary Association guidelines the test set. (b) Positive predictive value (PPV). The median PPV of the AI model, compared with the reference lymphocytes, was lower for 
intraepithelial lymphocytes than lamina propria lymphocytes in the validation regions of the test set. (c) F1 score (the harmonic mean of 
sensitivity and PPV). The median F1 score of the AI model, compared with the reference lymphocytes for the intraepithelial lymphocytes 
and lamina propria lymphocytes were similar in the validation regions of the test set, reflecting a higher sensitivity for the intraepithelial 
lymphocytes than the lamina propria lymphocytes, counterbalanced by a higher PPV for the lamina propria lymphocytes than the intraepithelial 
lymphocytes.
Figure 8. Positive predictive value (PPV) by sensitivity at different confidence thresholds. Median PPV and sensitivity for the artificial 
intelligence (AI) model compared with the reference lymphocytes was calculated using the range of encountered AI model confidence values. 
The increase in median PPV observed with higher AI model confidence thresholds was modest compared with the decrease in median 
sensitivity. false positive was defined as an AI-generated lymphocyte pre-
diction that was not supported by a reference lymphocyte. A 
false positive was considered “unequivocal” if it was not iden-
tified by any pathologist. A false negative was defined as a ref-
erence lymphocyte that was not detected by the model. A false 
negative was deemed “unequivocal” if all pathologists identi-
fied the reference lymphocyte. Equivocal lymphocytes that 
were not detected by the model were classified as true nega-
tives (Fig. 4). Sensitivity (also known as recall), positive pre-
dictive value (PPV , also known as precision), and F1 scores 
(harmonic mean of sensitivity and PPV) were calculated indi-
vidually for each validation region, averaged across the test set, 
and reported as median and interquartile range (IQR) values.
To understand causes of error, unequivocal false negative 
and false positive lymphocytes were classified by a board-cer-
tified veterinary anatomic pathologist (JMW) as caused by cell 
detection model errors, mucosal compartment model errors, or 
technical validation errors (mispredictions of cells located on 
the boundary of the validation region or cases where precise 
reference lymphocyte localization failed).
Interobserver Agreement Between  
Pathologists at the WSI Level
Eleven pathologists semiquantitatively graded intraepithelial 
lymphocytes as well as lamina propria lymphocytes and plasma 
cells according to the 2008 World Small Animal Veterinary 
Association guidelines.4 The pathologists evaluated all sections 
available in the WSI and provided a grade for intraepithelial 
Figure 4. Workflow for reference lymphocyte identification. (a) Lymphocyte nuclei were annotated by 11 pathologists (grey dots). (b) Individual 
pathologist annotations were clustered into candidate lymphocytes (circles) based on a minimum distance threshold (see Supplemental Materials). Wulcan et al 149
important question. The higher the required level of annotator 
agreement, the fewer the identified reference lymphocytes, the 
higher the model sensitivity but the lower the PPV . Requiring 
complete annotator agreement for reference lymphocyte deter-
mination yielded a high sensitivity but low PPV and likely 
underestimated the true number of lymphocytes. Instead, we 
used a majority decision for reference lymphocyte determina-
tion, which balanced sensitivity and PPV and represents a well-
established method for resolving rater discrepancies.6 An 
alternative approach could have been to use immunohisto-
chemistry-labeled lymphocytes as the reference standard, but 
this was deemed infeasible for a study of this size. In addition, 
this strategy might have resulted in greater discrepancies 
between the number of reference lymphocytes on one hand and 
the number of AI-predicted or human-annotated lymphocytes 
on the other hand as, in the authors’ opinion, humans tend to 
underestimate the number of CD3-positive cells based on H&E 
slides.
Most false positives observed in our study stemmed from the 
model detecting equivocal lymphocytes, that is, lymphocytes 
that were identified by fewer than six pathologists. A less con-
servative validation approach might have classified these as true 
positives. Of greater concern were unequivocal false positives, 
that is, AI-predicted lymphocytes that were not annotated by 
any pathologist. Enterocyte nuclei, plasma cells and goblet cell 
nuclei were identified as sources of confusion within this cate-
gory. A comprehensive evaluation of the AI model’s ability to 
differentiate between different cell types would require the 
annotation of various cell types, a task beyond the study’s scope.
Instances of unequivocal false negatives were infrequent in 
our study and primarily occurred in WSIs with faded stain or 
suboptimal focus. Our model development strategy leveraged a